{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Data in Python\n",
    "<b>df[\"col1\"] = df[\"col1\"].str.strip(\"s\")</b>In the column col1 strip the symbol 's' from all values. Data type is still object(due to dollor sign) so need to convert to int/float. <br></br>\n",
    "<b>df[\"col1\"] = df[\"col1\"].astype(\"int\")</b> Convert column col1 into int data type. <br></br>\n",
    "<b>assert df[\"col1\"].dtype == \"int\"</b> Returns none if statement is true and error if false. <br></br>\n",
    "<b>df[\"col1\"] = df[\"col1\"].astype(\"category\")</b> Numbers representing category(for example booleans or binary) get data type \"category\" for proper summary of values <br></br>\n",
    "Droping data that are out of range:\n",
    "<b>var = df[df[\"col1\"] <= 5]</b> Drop values using filtering where all values over 5 and under will be kept <br></br>\n",
    "<b>df.drop(df[df[\"col1\"] > 5].index, inplace=True)</b> Drop values using drop() where all values above 5 will be dropped. inplace=True dropped in place so we don't have to create new column <br></br>\n",
    "<b>assert df[\"col1\"].max() <= 5</b> Returns nothing if statement is true and AssertionError if false <br></br>\n",
    "<b>df.loc[df[\"col1\"] > 5, \"col1\"] = 5</b> Converts all values greater than 5 into 5. First argumnet is row index and second is column index <br></br>\n",
    "<b>df[\"col2\"] = pd.to_datetime(df[\"col2\"]).dt.date</b> convert col2 from object into pandas datetime object<br></br>\n",
    "<b>today_date = dt.date.today()</b> Saves todays date to variable today_date <br></br>\n",
    "Handling duplicate values:\n",
    "<b>duplicates = df.duplicated(column_names_var, keep=False)</b> Get duplicates across all columns listed in column_names_var, and keep all of them(do not drop)  <br></br>\n",
    "<b>df[duplicates].sort_values(by=\"col1\")</b> Easier to see duplicate values if they are sorted <br></br>\n",
    "<b>df.drop_duplicates(inplace=True)</b> drops complete duplicates in the whole dataframe since no columns where listed. inplace=True drops duplicated rows directly inside the DF w/out creating new object <br></br>\n",
    "<b>summaries = {\"height\": \"max\", \"weight\": \"mean\"}</b> To be used in agg function below <br></br>\n",
    "<b>var = df.groupby(by=column_names_var).agg(summaries).reset_index</b> Chains agg function to columns grouped by column_names_var and use .reset_index() to have numbered indices in the final output <br></br>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the information of ride_sharing\n",
    "print(ride_sharing.info())\n",
    "\n",
    "# Print summary statistics of user_type column\n",
    "print(ride_sharing['user_type'].describe())\n",
    "\n",
    "# Convert user_type from integer to category\n",
    "ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype(\"category\")\n",
    "\n",
    "# Write an assert statement confirming the change\n",
    "assert ride_sharing['user_type_cat'].dtype == 'category'\n",
    "\n",
    "# Print new summary statistics \n",
    "print(ride_sharing['user_type_cat'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip duration of minutes\n",
    "ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip(\"minutes\")\n",
    "\n",
    "# Convert duration to integer\n",
    "ride_sharing['duration_time'] = ride_sharing[\"duration_trim\"].astype(\"int\")\n",
    "\n",
    "# Write an assert statement making sure of conversion\n",
    "assert ride_sharing['duration_time'].dtype == 'int'\n",
    "\n",
    "# Print formed columns and calculate average ride duration \n",
    "print(ride_sharing[['duration','duration_trim','duration_time']])\n",
    "print(ride_sharing[\"duration_time\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tire_sizes to integer\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')\n",
    "\n",
    "# Set all values above 27 to 27\n",
    "ride_sharing.loc[ride_sharing[\"tire_sizes\"] > 27, \"tire_sizes\"] = 27\n",
    "\n",
    "# Reconvert tire_sizes back to categorical\n",
    "ride_sharing['tire_sizes'] = ride_sharing[\"tire_sizes\"].astype(\"category\")\n",
    "\n",
    "# Print tire size description\n",
    "print(ride_sharing['tire_sizes'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicates\n",
    "duplicates = ride_sharing.duplicated(\"ride_id\", keep=False)\n",
    "\n",
    "# Sort your duplicated rides\n",
    "duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')\n",
    "\n",
    "# Print relevant columns of duplicated_rides\n",
    "print(duplicated_rides[['ride_id','duration','user_birth_year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop complete duplicates from ride_sharing\n",
    "ride_dup = ride_sharing.drop_duplicates()\n",
    "\n",
    "# Create statistics dictionary for aggregation function\n",
    "statistics = {'user_birth_year': \"min\", 'duration': \"mean\"}\n",
    "\n",
    "# Group by ride_id and compute new statistics\n",
    "ride_unique = ride_dup.groupby('ride_id').agg(statistics).reset_index()\n",
    "\n",
    "# Find duplicated values again\n",
    "duplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)\n",
    "duplicated_rides = ride_unique[duplicates == True]\n",
    "\n",
    "# Assert duplicates are processed\n",
    "assert duplicated_rides.shape[0] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text and categorical data problems\n",
    "Always keep a log of possible categorical data to make dealing with inconsistencies easier. <br></br>\n",
    "Inner join keeps whats in both DF A and B while anti join keeps only whats in A and not in B(no overlapping) <br></br>\n",
    "<b>var_inconsistent = set(df1[\"col1\"]).difference(df_log[\"col1\"])</b> Set stores all unique values and difference compares if there are any different values between df1 and df_log. We can tell if there are any categories in df1 that are not listed in df_log <br></br>\n",
    "<b>var_rows = df[\"col1\"].isin(var_inconsistent)</b>Find all rows that have inconsistent categories. Returns T/F <br></br>\n",
    "<b>df[var_inconsistent]</b> Subset data to get values <br></br>\n",
    "<b>var = df[~var_inconsistent]</b> Drops inconsistent categories and keeps consistent data only <br></br>\n",
    "Value inconsistency, collapsing too many categories to few: <br></br>\n",
    "<b>df[\"col\"].value_counts()</b> Works on series only DF.groupby(\"col\").counts() works on DF\n",
    "<b>df[\"col1\"] = df[\"col1\"].str.upper()</b> Making every row in col1 upper case. Fixing 2 rows that have different ways of spelling the same category. <br></br>\n",
    "<b>.str.lower()</b> Makes every value in column lower case <br></br>\n",
    "<b>var = df[\"col1\"].str.strip()</b> With no argumnet given strip method removes all trailing and leading white spaces <br></br>\n",
    "<b>df[\"new_name\"] = pd.cut([\"col1\"], bins=[0,20, 50, np.inf], labels=[\"0-20k\", \"20-50k\", \"50K+\"]). cut the data into 3 groups with the names defined in labels. Use the range defined in bins. <br></br>\n",
    "<b>mapping = {\"name0\": \"cat1\",\"name1\": \"cat1\",\"name2\": \"cat0\",\"name3\": \"cat1\",\"name4\": \"cat0\"}</b> Will be used below <br></br>\n",
    "<b>df[\"col1\"] = df[\"col1\"].replace(mapping)</b> Replace col1 keys with its corresponding values. Column values should now be name1 or name2. <br></br>\n",
    "<b>assert df[\"col1\"].str.contains(\"+|-\").any() == False</b> Check if col1 contains + or - in any of its values. <br></br>\n",
    "<b>var_len = df[\"col1\"].str.len()</b> Used below <br></br>\n",
    "<b>df.loc[var_len < 10, \"col1\"] = np.nan</b> Convert values in col1 to NaN if the length of the value is less than 10 <br></br>\n",
    "Regular expressions:\n",
    "<b>df[\"col1\"] = df[\"col1\"].str.replace(r'<br></br>D+', \"\")</b> This means replace everything that is not a digit with nothing <br></br>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print categories DataFrame\n",
    "print(categories)\n",
    "\n",
    "# Print unique values of survey columns in airlines\n",
    "print('Cleanliness: ', airlines['cleanliness'].unique(), \"\\n\")\n",
    "print('Safety: ', airlines[\"safety\"].unique(), \"\\n\")\n",
    "print('Satisfaction: ', airlines[\"satisfaction\"].unique(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the cleanliness category in airlines not in categories\n",
    "cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
    "\n",
    "# Find rows with that category\n",
    "cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
    "\n",
    "# Print rows with inconsistent category\n",
    "print(airlines[cat_clean_rows])\n",
    "\n",
    "# Print rows with consistent categories only\n",
    "print(airlines[~cat_clean_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print unique values of both columns\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())\n",
    "\n",
    "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
    "airlines['dest_region'] = airlines['dest_region'].str.lower() \n",
    "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})\n",
    "\n",
    "# Remove white spaces from `dest_size`\n",
    "airlines['dest_size'] = airlines['dest_size'].str.strip()\n",
    "\n",
    "# Verify changes have been effected\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ranges for categories\n",
    "label_ranges = [0, 60, 180, np.inf]\n",
    "label_names = ['short', \"medium\", \"long\"]\n",
    "\n",
    "# Create wait_type column\n",
    "airlines['wait_type'] = pd.cut(airlines[\"wait_min\"], bins = label_ranges, \n",
    "                                labels = label_names)\n",
    "\n",
    "# Create mappings and replace\n",
    "mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', \n",
    "            'Thursday': 'weekday', 'Friday': 'weekday', \n",
    "            'Saturday': 'weekend', 'Sunday': 'weekend'}\n",
    "\n",
    "airlines['day_week'] = airlines['day'].replace(mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"Dr.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Dr.\", \"\")\n",
    "\n",
    "# Replace \"Mr.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Mr.\", \"\")\n",
    "\n",
    "# Replace \"Miss\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Miss\", \"\")\n",
    "\n",
    "# Replace \"Ms.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Ms.\", \"\")\n",
    "\n",
    "# Assert that full_name has no honorifics\n",
    "assert airlines['full_name'].str.contains('Ms.|Mr.|Miss|Dr.').any() == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store length of each row in survey_response column\n",
    "resp_length = airlines[\"survey_response\"].str.len()\n",
    "\n",
    "# Find rows in airlines where resp_length > 40\n",
    "airlines_survey = airlines[resp_length > 40]\n",
    "\n",
    "# Assert minimum survey_response length is > 40\n",
    "assert airlines_survey[\"survey_response\"].str.len().min() > 40\n",
    "\n",
    "# Print new survey_response column\n",
    "print(airlines_survey['survey_response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Data Problems\n",
    "Unit Uniformity problem:\n",
    "<b>var_fah = temp.loc[temp[\"col1\"] > 40, \"col1\"]</b> Used below <br></br>\n",
    "<b>temp_cels = (var_fah -32) * (5/9)</b> Used below <br></br>\n",
    "<b>temp.loc[temp[\"col1\"] > 40, \"col1\"] = temp _cels</b> Convert unit from F to C <br></br>\n",
    "<b>df[\"col1\"] = pd.to_datetime(df[\"col1\"], infer_datetime_format=True, errors=\"coerce\")</b> Change dates in col1 to pandas datetime. Will infer the format and return NaT for missing values instead of an error being raised. <br></br>\n",
    "<b>df[\"col1\"] = df[\"col1\"].dt.strftime(\"%d-%m-%Y\")</b> <br></br>\n",
    "Cross field validation:\n",
    "<b>var_sum = df[[\"col1\"], \"col2\", \"col3\"]].sum(axis=1)</b> Used below <br></br>\n",
    "<b>var_check = var_sum == df[\"col4\"]</b> Check if the sum of the first three columns is the same as what the DF displays.<br></br>\n",
    "Missing Data: <br></br>\n",
    "<b>df.isna().sum()</b> Checks if there is any missing data in df. Adding .sum() shows how many na values there are in df <br></br>\n",
    "<b>import missingno as msno</b> Used to visualize missing data <br></br>\n",
    "<b>df.dropna(subset=[\"col1\"])</b> drops na values in col1 <br></br>\n",
    "<b>df.fillna({\"col1\": var_mean})</b> Fill na values with mean values defined in var_mean <br></br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find values of acct_cur that are equal to 'euro'\n",
    "acct_eu = banking['acct_cur'] == 'euro'\n",
    "\n",
    "# Convert acct_amount where it is in euro to dollars\n",
    "banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1 \n",
    "\n",
    "# Unify acct_cur column by changing 'euro' values to 'dollar'\n",
    "banking.loc[acct_eu, 'acct_cur'] = 'dollar'\n",
    "\n",
    "# Assert that only dollar currency remains\n",
    "assert banking['acct_cur'].unique() == 'dollar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the header of account_opend\n",
    "print(banking['account_opened'].head())\n",
    "\n",
    "# Convert account_opened to datetime\n",
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
    "                                           # Infer datetime format\n",
    "                                           infer_datetime_format = True,\n",
    "                                           # Return missing value for error\n",
    "                                           errors = 'coerce') \n",
    "\n",
    "# Get year of account opened\n",
    "banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')\n",
    "\n",
    "# Print acct_year\n",
    "print(banking[\"acct_year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store fund columns to sum against\n",
    "fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']\n",
    "\n",
    "# Find rows where fund_columns row sum == inv_amount\n",
    "inv_equ = banking[fund_columns].sum(axis=1) == banking[\"inv_amount\"]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_inv = banking[inv_equ]\n",
    "inconsistent_inv = banking[~inv_equ] \n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent investments: \", inconsistent_inv.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store today's date and find ages\n",
    "today = dt.date.today()\n",
    "ages_manual = today.year - banking['birth_date'].dt.year\n",
    "\n",
    "# Find rows where age column == ages_manual\n",
    "age_equ = banking['age'] == ages_manual\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_ages = banking[age_equ]\n",
    "inconsistent_ages = banking[~age_equ]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent ages: \", inconsistent_ages.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record linkage\n",
    "Collapsing incorrect categories with string similarity <br></br>\n",
    "<b>for var in df_correct_cat[\"col1\"]:</b> <br></br> \n",
    "<b>var_name = process.extract(var, df[\"col1\"], limit = df.shape[0])</b> df_correct_cat contains the correct categories that df will ba collapsed to. this line will find potential matches in df with typoes. <br></br>\n",
    "<b>for potential_match in matches:</b> <br></br>\n",
    "<b>if potential_match[1] >= 80: </b> <br></br> if the string similarity score is high then ... <br></br>\n",
    "<b>df.loc[\"col1\"] == potential_match[0], 'col1'] = var</b> Replaces typo with correct category <br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f51fba3d43c57b17b163e71fbbb3db01d8855d2be5857b3f133c22d8c8ed697"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
