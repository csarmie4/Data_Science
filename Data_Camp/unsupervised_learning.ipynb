{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsupervised Learning in Python\n",
    "<b>from sklearn.cluster import KMeans</b> Used below \\\n",
    "<b>model = KMeans(n_clusters=3)</b> Create kmeans model with 3 clusters \\\n",
    "<b>model.fit(samples)</b> Fit the model on a sample array called samples \\\n",
    "<b>labels = model.predict(samples)</b> Make predictions and returns a cluster label for each sample indicating to which cluster the sample belongs \\\n",
    "<b>plt.scatter(x, y, color=labels)</b> Plot clusters \\\n",
    "Cross tabulation to evaluate clustering: \\\n",
    "<b>df = pd.DataFrame({\"labels\": labels, \"species\", species})</b> Used below \\\n",
    "<b>ct = pd.crosstab(df[\"labels\"], df[\"species\"])</b> Creates cross tabulation that shows which samples are in which cluster \\\n",
    "Inertia measures clustering quality. Lower inertia is better \\\n",
    "<b>model.inertia_</b> Prints how spread out the clusters are or distance from each sample to the centroid of cluster \\\n",
    "If features have too much variance a good method is standardize the features \\\n",
    "<b>from sklearn.preprocessing import StandardScaler</b> Used below \\\n",
    "<b>StandardScaler(copy=True, with_mean=True, with_std=True)</b> \\\n",
    "sample_scaled = scaler.transform(samples)</b> Can use this on new samples \\\n",
    "Use fit() / transform() with StandardScaler b/c it transform the data \\\n",
    "Use fit() / predict() with KMeans \\\n",
    "<b>pipeline = make_pipeline(scaler, kmeans)</b> \\\n",
    "<b>pipeline.fit(samples)</b> \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create a KMeans instance with 3 clusters: model\n",
    "model = KMeans(n_clusters=3)\n",
    "\n",
    "# Fit model to points\n",
    "model.fit(points)\n",
    "\n",
    "# Determine the cluster labels of new_points: labels\n",
    "labels = model.predict(new_points)\n",
    "\n",
    "# Print cluster labels of new_points\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assign the columns of new_points: xs and ys\n",
    "xs = new_points[:, 0]\n",
    "ys = new_points[:, 1]\n",
    "\n",
    "# Make a scatter plot of xs and ys, using labels to define the colors\n",
    "plt.scatter(xs, ys, c=labels, alpha=0.5)\n",
    "\n",
    "# Assign the cluster centers: centroids\n",
    "centroids = model.cluster_centers_\n",
    "\n",
    "# Assign the columns of centroids: centroids_x, centroids_y\n",
    "centroids_x = centroids[:,0]\n",
    "centroids_y = centroids[:,1]\n",
    "\n",
    "# Make a scatter plot of centroids_x and centroids_y\n",
    "plt.scatter(centroids_x, centroids_y, marker=\"D\", s=50)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1, 6)\n",
    "inertias = []\n",
    "\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(samples)\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "# Plot ks vs inertias\n",
    "plt.plot(ks, inertias, '-o')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KMeans model with 3 clusters: model\n",
    "model = KMeans(n_clusters=3)\n",
    "\n",
    "# Use fit_predict to fit model and obtain cluster labels: labels\n",
    "labels = model.fit_predict(samples)\n",
    "\n",
    "# Create a DataFrame with clusters and varieties as columns: df\n",
    "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
    "\n",
    "# Create crosstab: ct\n",
    "ct = pd.crosstab(df['labels'], df['varieties'])\n",
    "\n",
    "# Display ct\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create scaler: scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create KMeans instance: kmeans\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Fit the pipeline to samples\n",
    "pipeline.fit(samples)\n",
    "\n",
    "# Calculate the cluster labels: labels\n",
    "labels = pipeline.predict(samples)\n",
    "\n",
    "# Create a DataFrame with labels and species as columns: df\n",
    "df = pd.DataFrame({\"labels\": labels, \"species\": species})\n",
    "\n",
    "# Create crosstab: ct\n",
    "ct = pd.crosstab(df[\"labels\"], df[\"species\"])\n",
    "\n",
    "# Display ct\n",
    "print(ct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some stocks are more expensive than others. To account for this, include a Normalizer at the beginning of your pipeline. The Normalizer will separately transform each company's stock price to a relative scale before the clustering begins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Normalizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Create a normalizer: normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# Create a KMeans model with 10 clusters: kmeans\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "\n",
    "# Make a pipeline chaining normalizer and kmeans: pipeline\n",
    "pipeline = make_pipeline(normalizer, kmeans)\n",
    "\n",
    "# Fit pipeline to the daily price movements\n",
    "pipeline.fit(movements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Predict the cluster labels: labels\n",
    "labels = pipeline.predict(movements)\n",
    "\n",
    "# Create a DataFrame aligning labels and companies: df\n",
    "df = pd.DataFrame({'labels': labels, 'companies': companies})\n",
    "\n",
    "# Display df sorted by cluster label\n",
    "print(df.sort_values(\"labels\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization with hierarchical clustering and t-SNE\n",
    "<b>from scipy.cluster.hierarchy import linkage, dendrogram</b> Used below \\\n",
    "<b>mergins = linkage(samples, method='complete')</b> 'complete' linkage is distance between clusters is max distance between their samples. linkage() function obtains a hierarchical clustering of samples \\\n",
    "<b>dendrogram(mergings, labels=country_names)</b> dendrogram() visualizes the result\n",
    "<b>from scipy.cluster.hierarchy import fcluster</b> \\\n",
    "<b>labels = fcluster(merging, 15, criterion=\"distance\")</b> Returns an array with cluster labels for all samples for a height of 15. \\\n",
    "<b>from sklearn.manifold import TSNE</b> Used below \\\n",
    "<b>model = TSNE(learning_rate=100)</b> Try different learning rates between around 50-200 Create TSNE object \\\n",
    "<b>transformed = model.fit_transform(samples)</b> fit_transform fits and transforms the samples in one method call \\\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the linkage: mergings\n",
    "mergings = linkage(samples, method='complete')\n",
    "\n",
    "# Plot the dendrogram, using varieties as labels\n",
    "dendrogram(mergings,\n",
    "           labels=varieties,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=6,\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import normalize\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Normalize the movements: normalized_movements\n",
    "normalized_movements = normalize(movements)\n",
    "\n",
    "# Calculate the linkage: mergings\n",
    "mergings = linkage(normalized_movements, method=\"complete\")\n",
    "\n",
    "# Plot the dendrogram\n",
    "dendrogram(mergings, labels=companies, leaf_rotation=90, leaf_font_size=6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "# Calculate the linkage: mergings\n",
    "mergings = linkage(samples, method='single')\n",
    "\n",
    "# Plot the dendrogram\n",
    "dendrogram(mergings,\n",
    "           labels=country_names,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=6,\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "performing single linkage hierarchical clustering produces a different dendrogram!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "# Use fcluster to extract labels: labels\n",
    "labels = fcluster(mergings, 6, criterion='distance')\n",
    "\n",
    "# Create a DataFrame with labels and varieties as columns: df\n",
    "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
    "\n",
    "# Create crosstab: ct\n",
    "ct = pd.crosstab(df['labels'], df['varieties'])\n",
    "\n",
    "# Display ct\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TSNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Create a TSNE instance: model\n",
    "model = TSNE(learning_rate=200)\n",
    "\n",
    "# Apply fit_transform to samples: tsne_features\n",
    "tsne_features = model.fit_transform(samples)\n",
    "\n",
    "# Select the 0th feature: xs\n",
    "xs = tsne_features[:,0]\n",
    "\n",
    "# Select the 1st feature: ys\n",
    "ys = tsne_features[:,1]\n",
    "\n",
    "# Scatter plot, coloring by variety_numbers\n",
    "plt.scatter(xs, ys, c=variety_numbers)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TSNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Create a TSNE instance: model\n",
    "model = TSNE(learning_rate=50)\n",
    "\n",
    "# Apply fit_transform to normalized_movements: tsne_features\n",
    "tsne_features = model.fit_transform(normalized_movements)\n",
    "\n",
    "# Select the 0th feature: xs\n",
    "xs = tsne_features[:,0]\n",
    "\n",
    "# Select the 1th feature: ys\n",
    "ys = tsne_features[:,1]\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(xs, ys, alpha=0.5)\n",
    "\n",
    "# Annotate the points\n",
    "for x, y, company in zip(xs, ys, companies):\n",
    "    plt.annotate(company, (x, y), fontsize=5, alpha=0.75)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decorrelating your data and dimension reduction\n",
    "Dimension reduction: More efficient storage and computation because it removes less-informative \"noise\" features \\\n",
    "PCA: Principal component analysis is a dimension reduction technique. \\\n",
    "Principal components = direction of variance\n",
    "First step: PCA aligns data with axes and results in decorrelation(not lineraly correlated anymore) \\\n",
    "<b>from scipy.stats import pearsonr</b> Used below \\\n",
    "<b>pearsonr(x, y)</b> -1, 1 Used to measure correlation between x and y \\\n",
    "\n",
    "<b>from sklearn.decomposition import PCA</b> Used below \\\n",
    "Work flow: PCA(), .fit(), .transform() \\\n",
    "<b>range(model.n_components_)</b> Creates range enumerating the PCA features \\\n",
    "<b>features = pca.explained_variance_</b> Where the variance can be accessed of the PCA model \\\n",
    "<b>plt.bar(features, pca.explained_variance_)</b> To plt variance \\\n",
    "<b>PCA(n_components=2)</b> fit(), tansform(), Only keep 2 components/dimensions \\\n",
    "PCA assumes high variance info as informative and low variance as noise to be discarded. \\\n",
    "Not always true, for example word frequency arrays and are called \"Sparse\": most entries are zero \\\n",
    "Can use scipy.sparse.csr_matrix instead of numpy array. csr_matrix remembers only the non-zero entries which saves space \\\n",
    "PCA does not support csr_matrix have to use TruncatedSVD instead. They perform and can be interacted with the same way as PCA \\\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Assign the 0th column of grains: width\n",
    "width = grains[:, 0]\n",
    "\n",
    "# Assign the 1st column of grains: length\n",
    "length = grains[:, 1]\n",
    "\n",
    "# Scatter plot width vs length\n",
    "plt.scatter(width, length)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the Pearson correlation\n",
    "correlation, pvalue = pearsonr(width, length)\n",
    "\n",
    "# Display the correlation\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create PCA instance: model\n",
    "model = PCA()\n",
    "\n",
    "# Apply the fit_transform method of model to grains: pca_features\n",
    "pca_features = model.fit_transform(grains)\n",
    "\n",
    "# Assign 0th column of pca_features: xs\n",
    "xs = pca_features[:,0]\n",
    "\n",
    "# Assign 1st column of pca_features: ys\n",
    "ys = pca_features[:,1]\n",
    "\n",
    "# Scatter plot xs vs ys\n",
    "plt.scatter(xs, ys)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the Pearson correlation of xs and ys\n",
    "correlation, pvalue = pearsonr(xs, ys)\n",
    "\n",
    "# Display the correlation\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scatter plot of the untransformed points\n",
    "plt.scatter(grains[:,0], grains[:,1])\n",
    "\n",
    "# Create a PCA instance: model\n",
    "model = PCA()\n",
    "\n",
    "# Fit model to points\n",
    "model.fit(grains)\n",
    "\n",
    "# Get the mean of the grain samples: mean\n",
    "mean = model.mean_\n",
    "\n",
    "# Get the first principal component: first_pc\n",
    "first_pc = model.components_[0, :]\n",
    "\n",
    "# Plot first_pc as an arrow, starting at mean\n",
    "plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)\n",
    "\n",
    "# Keep axes on same scale\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create scaler: scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create a PCA instance: pca\n",
    "pca = PCA()\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, pca)\n",
    "\n",
    "# Fit the pipeline to 'samples'\n",
    "pipeline.fit(samples)\n",
    "\n",
    "# Plot the explained variances\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_)\n",
    "plt.xlabel('PCA feature')\n",
    "plt.ylabel('variance')\n",
    "plt.xticks(features)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA model with 2 components: pca\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit the PCA instance to the scaled samples\n",
    "pca.fit(scaled_samples)\n",
    "\n",
    "# Transform the scaled samples: pca_features\n",
    "pca_features = pca.transform(scaled_samples)\n",
    "\n",
    "# Print the shape of pca_features\n",
    "print(pca_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TfidfVectorizer: tfidf\n",
    "tfidf = TfidfVectorizer() \n",
    "\n",
    "# Apply fit_transform to document: csr_mat\n",
    "csr_mat = tfidf.fit_transform(documents)\n",
    "\n",
    "# Print result of toarray() method\n",
    "print(csr_mat.toarray())\n",
    "\n",
    "# Get the words: words\n",
    "words = tfidf.get_feature_names()\n",
    "\n",
    "# Print words\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create a TruncatedSVD instance: svd\n",
    "svd = TruncatedSVD(n_components=50)\n",
    "\n",
    "# Create a KMeans instance: kmeans\n",
    "kmeans = KMeans(n_clusters=6)\n",
    "\n",
    "# Create a pipeline: pipeline\n",
    "pipeline = make_pipeline(svd, kmeans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Fit the pipeline to articles\n",
    "pipeline.fit(articles)\n",
    "\n",
    "# Calculate the cluster labels: labels\n",
    "labels = pipeline.predict(articles)\n",
    "\n",
    "# Create a DataFrame aligning labels and titles: df\n",
    "df = pd.DataFrame({'label': labels, 'article': titles})\n",
    "\n",
    "# Display df sorted by cluster label\n",
    "print(df.sort_values('label'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discovering interpretable features\n",
    "NMF: \"non-negative matrix factorization\" is a dimension reduction technique like PCA but they are interpretable(easier to explain) but samples have to be non-negative \\\n",
    "NMF must specify number of componentss and works with numpy arrays and csr_matrix \\\n",
    "<b>from sklearn.decomposition import NMF</b> Used below \\\n",
    "<b>NMF(n_components=2)</b> fit(), tansform(), Only keep 2 components/dimensions \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NMF\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Create an NMF instance: model\n",
    "model = NMF(n_components=6)\n",
    "\n",
    "# Fit the model to articles\n",
    "model.fit(articles)\n",
    "\n",
    "# Transform the articles: nmf_features\n",
    "nmf_features = model.transform(articles)\n",
    "\n",
    "# Print the NMF features\n",
    "print(nmf_features.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Create a pandas DataFrame: df\n",
    "df = pd.DataFrame(nmf_features, index=titles)\n",
    "\n",
    "# Print the row for 'Anne Hathaway'\n",
    "print(df.loc[\"Anne Hathaway\"])\n",
    "\n",
    "# Print the row for 'Denzel Washington'\n",
    "print(df.loc[\"Denzel Washington\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame: components_df\n",
    "components_df = pd.DataFrame(model.components_, columns=words)\n",
    "\n",
    "# Print the shape of the DataFrame\n",
    "print(components_df.shape)\n",
    "\n",
    "# Select row 3: component\n",
    "component = components_df.iloc[3]\n",
    "\n",
    "# Print result of nlargest\n",
    "print(component.nlargest())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Select the 0th row: digit\n",
    "digit = samples[0,:]\n",
    "\n",
    "# Print digit\n",
    "print(digit)\n",
    "\n",
    "# Reshape digit to a 13x8 array: bitmap\n",
    "bitmap = digit.reshape((13, 8))\n",
    "\n",
    "# Print bitmap\n",
    "print(bitmap)\n",
    "\n",
    "# Use plt.imshow to display bitmap\n",
    "plt.imshow(bitmap, cmap='gray', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NMF\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Create an NMF model: model\n",
    "model = NMF(n_components=7)\n",
    "\n",
    "# Apply fit_transform to samples: features\n",
    "features = model.fit_transform(samples)\n",
    "\n",
    "# Call show_as_image on each component\n",
    "for component in model.components_:\n",
    "    show_as_image(component)\n",
    "\n",
    "# Select the 0th row of features: digit_features\n",
    "digit_features = features[0,:]\n",
    "\n",
    "# Print digit_features\n",
    "print(digit_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA instance: model\n",
    "model = PCA(n_components=7)\n",
    "\n",
    "# Apply fit_transform to samples: features\n",
    "features = model.fit_transform(samples)\n",
    "\n",
    "# Call show_as_image on each component\n",
    "for component in model.components_:\n",
    "    show_as_image(component)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike NMF, PCA doesn't learn the parts of things. Its components do not correspond to topics (in the case of documents) or to parts of images, when trained on images. Verify this for yourself by inspecting the components of a PCA model fit to the dataset of LED digit images from the previous exercise. The images are available as a 2D array samples. Also available is a modified version of the show_as_image() function which colors a pixel red if the value is negative.\n",
    "\n",
    "After submitting the answer, notice that the components of PCA do not represent meaningful parts of images of LED digits!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae6913f073cddf3e8b34322761411161793bc241a1d60fd908bb8c5c008cc29b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
