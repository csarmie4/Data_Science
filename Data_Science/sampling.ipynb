{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias Any Stretch of the Imagination\n",
    "Population: Complete dataset <br></br>\n",
    "Sample: Subset of the dataset <br></br>\n",
    "Collecting data by the easiest method is convenience sampling and can lead to bias sampling <br></br>\n",
    "<b>np.random.beta(a=2, b=2, size=5000)</b> 5000 random numbers from 0 to 1(beta distribution) is created. Params named a and b <br></br> \n",
    "<b>np.random.normal(loc=2, scale=1.5, size=2)</b> loc and scale arguments set the mean and std of the distribution. <br></br>\n",
    "<b>df.sample(n=10, random_state=21)</b> 5 random samples from df are selected. Argument random_state is similar to setting a random seed <br></br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.77766237 6.55425374 5.23675971 ... 8.39423111 6.00488939 3.04319512]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate random numbers from a Uniform(-3, 3)\n",
    "uniforms = np.random.uniform(low=-3, high=3, size=5000)\n",
    "\n",
    "# Generate random numbers from a Normal(5, 2)\n",
    "normals = np.random.normal(loc=5, scale=2, size=5000)\n",
    "\n",
    "# Print normals\n",
    "print(normals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't get theory eyed\n",
    "Systematic sampling: A sample value is picked every 5 samples <br></br>\n",
    "<b>interval = var // var</b> Used below. Integer division, discards decimals <br></br>\n",
    "<b>df.loc[::interval]</b> Systematic sampling. Double colon means nothing for the first argument, nothing for the second, and jump by interval. It gets every interval value of the sequence sliced. <br></br>\n",
    "<b>shuffled= df.sample(frac=1)</b> Randomly shuffles the rows. Needed if doing systematic sampling and there is bias(patern) in the df. Doing this is the same as random sampling(.sample()) <br></br>\n",
    "<b>shuffled = shuffled.reset_index(drop=True).reset_index()</b> drop=True clears the previous row indexes and chaining another reset_index creates new indexes. <br></br> \n",
    "Filtering for a list of categories <br></br>\n",
    "<b>var_0 = df[\"col1\"].isin(list_var)</b> Used below. Subset before using var_1=df[var_0] <br></br>\n",
    "<b>var_sample = var_0.sample(frac=0.1, random_state=212)</b> Takes a 10 percent simple random sample of data set <br></br>\n",
    "<b>var_sample[\"col1\"].value_counts(normalize=True)</b> To see the proportions of values in the sample <br></br>\n",
    "Proportional stratified sampling: Makes sampling proportions similar to population proportions. <br></br>\n",
    "To do this use groupby method before sampling the data set <br></br>\n",
    "<b>var = df.groupby(\"col1\").sample(frac=0.3, random_state=212)</b> <br></br>\n",
    "Equal count stratified sampling <br></br>\n",
    "Each category will have the same proportions. To do this switch frac=0.1 to n=\"int\" <br></br>\n",
    "<b>var = var_0.groupby(\"col1\").sample(n=15, random_state=212)</b> Now each proportion is equal <br></br>\n",
    "Weighted random sampling <br></br>\n",
    "Can specify weights for any category to adjust relative probability <br></br>\n",
    "<b>condition = var_0[\"col1\"] == \"cat_value\"</b> Used below <br></br>\n",
    "<b>var_0[\"new_col\"] = np.where(condition, 2, 1)</b> Weight of 2 for rows that match condition and weight of 1 for the ones that do not. Now cat_value has a two time chance of being picked <br></br>\n",
    "<b>var_0 = var_0.sample(frac=0.1, weights=\"weight\")</b> Weights are now adjusted <br></br>\n",
    "Cluster sampling: <br></br>\n",
    "Use simple random sampling to pick soome subgroups then use simple random sampling on those subgroups <br></br>\n",
    "<b>var_pop = list(df[\"col1\"].unique)</b> Returns list of unique values <br></br>\n",
    "<b>var_sample = random.sample(var_pop, k=3)</b> Randomly selectes 3 subgroups <br></br>\n",
    "<b>condition = df[\"col1\"].isin(var_sample)</b> Used below <br></br> \n",
    "<b>df_cluster = df[condition]</b> Subset data set <br></br>\n",
    "<b>df_cluster[\"col1\"] = df_cluster[\"col1\"].cat.remove_unused_categories()</b> Removes levels with zero rows <br></br>\n",
    "Now we can do our sampling <br></br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of employees by Education level\n",
    "education_counts_pop = attrition_pop['Education'].value_counts(normalize=True)\n",
    "\n",
    "# Print education_counts_pop\n",
    "print(education_counts_pop)\n",
    "\n",
    "# Proportional stratified sampling for 40% of each Education group\n",
    "attrition_strat = attrition_pop.groupby('Education')\\\n",
    "\t.sample(frac=0.4, random_state=2022)\n",
    "\n",
    "# Calculate the Education level proportions from attrition_strat\n",
    "education_counts_strat = attrition_strat[\"Education\"].value_counts(normalize=True)\n",
    "\n",
    "# Print education_counts_strat\n",
    "print(education_counts_strat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 30 employees from each Education group\n",
    "attrition_eq = attrition_pop.groupby('Education')\\\n",
    "\t.sample(n=30, random_state=2022)      \n",
    "\n",
    "# Get the proportions from attrition_eq\n",
    "education_counts_eq = attrition_eq['Education'].value_counts(normalize=True) \n",
    "\n",
    "# Print the results\n",
    "print(education_counts_eq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot YearsAtCompany from attrition_pop as a histogram\n",
    "attrition_pop['YearsAtCompany'].hist(bins=np.arange(0, 41, 1))\n",
    "plt.show()\n",
    "\n",
    "# Sample 400 employees weighted by YearsAtCompany\n",
    "attrition_weight = attrition_pop.sample(n=400, weights=\"YearsAtCompany\")\n",
    "\n",
    "# Plot YearsAtCompany from attrition_weight as a histogram\n",
    "attrition_weight['YearsAtCompany'].hist(bins=np.arange(0, 41, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of unique JobRole values\n",
    "job_roles_pop = list(attrition_pop['JobRole'].unique())\n",
    "\n",
    "# Randomly sample four JobRole values\n",
    "job_roles_samp = random.sample(job_roles_pop, k=4)\n",
    "\n",
    "# Filter for rows where JobRole is in job_roles_samp\n",
    "jobrole_condition = attrition_pop['JobRole'].isin(job_roles_samp)\n",
    "attrition_filtered = attrition_pop[jobrole_condition]\n",
    "\n",
    "# Remove categories with no rows\n",
    "attrition_filtered['JobRole'] = attrition_filtered['JobRole'].cat.remove_unused_categories()\n",
    "\n",
    "# Randomly sample 10 employees from each sampled job role\n",
    "attrition_clust = attrition_filtered.groupby(\"JobRole\")\\\n",
    "    .sample(n=10, random_state=2022)\n",
    "\n",
    "# Print the sample\n",
    "print(attrition_clust)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The n's justify the means\n",
    "<b>sample_mean = df.sample(n=sample_size)[\"col2\"].mean()</b> point estimate <br></br>\n",
    "<b>rel_error_pct = 100 * abs(population_mean - sample_mean) / population mean</b> Relative error <br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a simple random sample of 50 rows, with seed 2022\n",
    "attrition_srs50 = attrition_pop.sample(n=50, random_state=2022)\n",
    "\n",
    "# Calculate the mean employee attrition in the sample\n",
    "mean_attrition_srs50 = attrition_srs50['Attrition'].mean()\n",
    "\n",
    "# Calculate the relative error percentage\n",
    "rel_error_pct50 = 100 * abs(mean_attrition_pop-mean_attrition_srs50) / mean_attrition_pop\n",
    "\n",
    "# Print rel_error_pct50\n",
    "print(rel_error_pct50)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f51fba3d43c57b17b163e71fbbb3db01d8855d2be5857b3f133c22d8c8ed697"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
